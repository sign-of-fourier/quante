{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96776a9-0fdb-4d11-9d77-39c03c1c5644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import quante\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e717911-c4ee-4c29-8187-88a52aacd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "pd.DataFrame(iris.data).to_csv('X.csv', index=False)\n",
    "with open('y.csv', 'w') as f:\n",
    "    f.write(','.join([str(x) for x in iris.target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38de09-d4a5-4e62-b21e-1b87909244d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def instance(arch):\n",
    "    \n",
    "        X = pd.read_csv('X.csv')\n",
    "        with open('y.csv') as f:\n",
    "            y = f.read().split(',')\n",
    "            \n",
    "        clf = MLPClassifier(solver='sgd', max_iter=500,\n",
    "                        hidden_layer_sizes=arch[:3], alpha=arch[3], random_state=1)\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            cvs = cross_val_score(clf, X, y)\n",
    "        return cvs\n",
    "        \n",
    "#    print(\"Call a single instance of MLP Classifier\") \n",
    "#    print(instance((3,3)))\n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[2, 16], [2, 4], [2, 16], [.01, .99]], kernel=DotProduct()+ WhiteKernel(), n_batches=100, n_processors=4, n_iterations=5)\n",
    "    \n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe709758-2937-4306-879a-7c3cedd64b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = session.summary()\n",
    "summary['iteration_id'] = iterations\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9e244-30f9-4cef-89be-577266fda4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = summary[['score', 'iteration_id']].groupby('iteration_id').mean().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9069a02-61b9-47f1-aec6-b4afd418e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_alpha=  [x[:3]+(round(x[3],3),) for x in summary['layers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a462130-9b81-44c1-afef-d92f86a7ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize =(13, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot(session.score_history)\n",
    "ax.set_xticklabels(rounded_alpha)\n",
    "plt.xticks(rotation=60)\n",
    "bx = plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96158d44-10ef-4ad9-b714-82e2e81e5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def instance(hp_parameter):\n",
    "    \n",
    "        X = pd.read_csv('X.csv')\n",
    "        with open('y.csv') as f:\n",
    "            y = f.read().split(',')\n",
    "            \n",
    "        clf = LogisticRegression(solver='saga', penalty = 'elasticnet', l1_ratio=hp_parameter[0], max_iter=200 )\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            cvs = cross_val_score(clf, X, y)\n",
    "        \n",
    "        return cvs\n",
    "        \n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[.05, .95]], kernel=DotProduct()+ WhiteKernel(), n_batches=7, n_processors=4, n_iterations=5)\n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57754c4a-c8cb-4080-8325-37980fb9bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def instance(hp_parameter):\n",
    "    \n",
    "        X = pd.read_csv('X.csv')\n",
    "        with open('y.csv') as f:\n",
    "            y = f.read().split(',')\n",
    "            \n",
    "        clf = LogisticRegression(solver='saga', penalty = 'elasticnet', l1_ratio=hp_parameter[0], max_iter=200 )\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            cvs = cross_val_score(clf, X, y)\n",
    "        \n",
    "        return cvs\n",
    "        \n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[.05, .95]], kernel=DotProduct()+ WhiteKernel(), n_batches=7, n_processors=4, n_iterations=5)\n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da219579-29af-4a8e-b5fc-08a88734caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = session.summary()\n",
    "summary['iteration_id'] = iterations\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af5140-d021-45ca-9d46-646604c49660",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = summary[['score', 'iteration_id']].groupby('iteration_id').mean().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf422ae-3363-4919-9138-692db08154fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4e705-e901-4232-8118-6dfc12e1dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs=None, arch=None, n_outputs=None):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        if n_inputs and arch and n_outputs:\n",
    "            self.make_arch(n_inputs, arch, n_outputs)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n",
    "    def make_arch(self, n_inputs, arch, n_outputs, inner_function='Relu', last_function = None):\n",
    "        layers = [torch.nn.Linear(n_inputs, arch[0])]\n",
    "        for a in range(len(arch)-1):\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Linear(arch[a], arch[a+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(arch[-1], n_outputs))\n",
    "    #    layers.append(torch.nn.Softmax(dim=1))                    # works better without this\n",
    "\n",
    "        self.stack = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3545a-6561-4407-8183-a4c674c7c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_loader:\n",
    "    def __init__(self,batch_pct):\n",
    "        self.X_train = pd.read_csv('X_train.csv')\n",
    "        self.X_test = pd.read_csv('X_test.csv')\n",
    "        self.y_train = pd.read_csv('y_train.csv')\n",
    "        self.y_test = pd.read_csv('y_test.csv')\n",
    "    \n",
    "        self.train_batch_size = int(np.floor(self.X_train.shape[0] * batch_pct))\n",
    "        self.test_batch_size = int(np.floor(self.X_test.shape[0] * batch_pct))\n",
    "        self.batch_pct = batch_pct\n",
    "    def get_batch(self,batch):\n",
    "        if batch < 0:\n",
    "            return self.X_test, self.y_test\n",
    "        else:\n",
    "            return self.X_train[(batch*self.train_batch_size):((batch+1)*self.train_batch_size)],\\\n",
    "                   self.y_train[(batch*self.train_batch_size):((batch+1)*self.train_batch_size)]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41f27e-56e2-43cc-8670-034bebf98367",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_batch_size = 1000\n",
    "    n_batches = 4\n",
    "    n_iterations = 10\n",
    "\n",
    "    def instance(arch):\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            \n",
    "        model = NeuralNetwork(arch[0], arch[1:4], n_outputs=arch[4])\n",
    "        model.to('cuda:0')\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "\n",
    "        loader = data_loader(.05)\n",
    "    \n",
    "        for i in range(n_iterations):\n",
    "            for batch in range(n_batches):\n",
    "                X_train, y_train = loader.get_batch(batch)\n",
    "\n",
    "                X_train_torch = torch.tensor(X_train.values, dtype=torch.float)\n",
    "                y_train_torch = torch.tensor(y_train.values, dtype=torch.float)\n",
    "\n",
    "                X = X_train_torch.to('cuda:0')\n",
    "                y = y_train_torch.to('cuda:0')\n",
    "\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "        X_test, y_test = loader.get_batch(-1)\n",
    "\n",
    "        model.eval()\n",
    "        X_test_torch = torch.tensor(X_test.values, dtype=torch.float)\n",
    "        y_test_torch = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "        X = X_test_torch.to('cuda:0')\n",
    "        y = y_test_torch.to('cuda:0')\n",
    "        pred = model(X)\n",
    "\n",
    "\n",
    "        return loss.item()#.detatch()\n",
    "\n",
    "\n",
    "\n",
    "    nn = instance((28*28, 128, 128, 64, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125a11d-064a-4e5f-ba63-a8ba3aa78a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f1193-822c-49b2-8d32-d25ea16d8aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    hp_tune = quante.carlo(instance, [[.05, .95]], kernel=DotProduct()+ WhiteKernel(), n_batches=7, n_processors=4, n_iterations=5)\n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
